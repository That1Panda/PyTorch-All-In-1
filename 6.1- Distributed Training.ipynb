{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet18().to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += predicted.eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=resnet18()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        correct += predicted.eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\\\n",
    "from torch.distributed import dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def dist_training_loop(rank,world_size,train_dataloader,model,criterion,optimizer):\n",
    "    dist.init_process_group(backend='gloo',rank=rank,world_size=world_size)\n",
    "    model.to(rank)\n",
    "    ddp_model = DDP(model,device_ids=[rank])\n",
    "    optimizer= optimizer(ddp_model.parameters(),lr=0.001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs=model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs,1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    world_size = 2\n",
    "    mp.spawn(dist_training_loop, args=(world_size,train_dataloader,model,criterion,optimizer), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 10\n",
    "split_size = 32  # Adjust according to batch size requirements\n",
    "N_EPOCHS = 10  # Example epoch count\n",
    "\n",
    "\n",
    "# Define a model based on AlexNet and split across two GPUs\n",
    "class TwoGPUAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=num_classes, split_size=split_size):\n",
    "        super(TwoGPUAlexNet, self).__init__()\n",
    "        self.split_size = split_size\n",
    "\n",
    "        # Load a pretrained AlexNet and modify for model parallelism\n",
    "        alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "        # Divide model parts across GPUs\n",
    "        self.features = alexnet.features.to(\"cuda:0\")\n",
    "        self.avgpool = alexnet.avgpool.to(\"cuda:0\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        ).to(\"cuda:1\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input batch across GPUs\n",
    "        splits = iter(x.split(self.split_size, dim=0))\n",
    "        s_next = next(splits)\n",
    "\n",
    "        # Forward pass for the first part on GPU 0\n",
    "        s_prev = self.features(s_next)\n",
    "        s_prev = self.avgpool(s_prev)\n",
    "        s_prev = s_prev.view(s_prev.size(0), -1).to(\"cuda:1\")  # Transfer to GPU 1\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        for s_next in splits:\n",
    "            # Process on GPU 1\n",
    "            s_prev = self.classifier(s_prev)\n",
    "            ret.append(s_prev)\n",
    "\n",
    "            # Forward pass for next part on GPU 0\n",
    "            s_prev = self.features(s_next.to(\"cuda:0\"))\n",
    "            s_prev = self.avgpool(s_prev)\n",
    "            s_prev = s_prev.view(s_prev.size(0), -1).to(\"cuda:1\")\n",
    "\n",
    "        # Final part\n",
    "        s_prev = self.classifier(s_prev)\n",
    "        ret.append(s_prev)\n",
    "\n",
    "        return torch.cat(ret)\n",
    "\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = TwoGPUAlexNet(num_classes=num_classes, split_size=split_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy dataset and dataloader setup for illustration\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = datasets.FakeData(transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.to(\"cuda:0\")  # Inputs on GPU 0\n",
    "        labels = labels.to(\"cuda:1\")  # Labels on GPU 1 (since outputs will be on GPU 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDP & Model Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple2GPUModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super().__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.net1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.to(self.dev1)\n",
    "        x = self.net2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def model_parallel_training(rank, world_size):\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    ddp_model = DDP(Simple2GPUModel(dev0, dev1), device_ids=[dev0, dev1])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    dist.destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
